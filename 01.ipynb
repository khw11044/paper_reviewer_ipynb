{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os \n",
    "import re\n",
    "import glob\n",
    "import streamlit as st\n",
    "import base64\n",
    "from markdownify import markdownify as markdown\n",
    "\n",
    "from utils.Classes import GraphState, LayoutAnalyzer\n",
    "from utils.funcs import *\n",
    "from utils.extracts import *\n",
    "from utils.crops import *\n",
    "from utils.creates import *\n",
    "from utils.save import save_results\n",
    "\n",
    "from utils.creates import create_text_trans_summary\n",
    "from utils.vectordb import build_db\n",
    "from utils.prompt import summary_prompt, map_prompt, trans_prompt\n",
    "\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "OPENAI_API_KEY = os.environ.get(\"UPSTAGE_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzer = LayoutAnalyzer(OPENAI_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = './papers/objectvla.pdf'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "state = GraphState(filepath=file_path, batch_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "총 페이지 수: 11\n",
      "분할 PDF 생성: ./papers/objectvla_0000_0009.pdf\n",
      "분할 PDF 생성: ./papers/objectvla_0010_0010.pdf\n"
     ]
    }
   ],
   "source": [
    "split_file_list = split_pdf(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'split_filepaths': ['./papers/objectvla_0000_0009.pdf',\n",
       "  './papers/objectvla_0010_0010.pdf']}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_file_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 그래프에 업데이트\n",
    "state.update(split_file_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'filepath': './papers/objectvla.pdf',\n",
       " 'batch_size': 10,\n",
       " 'split_filepaths': ['./papers/objectvla_0000_0009.pdf',\n",
       "  './papers/objectvla_0010_0010.pdf']}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PDF 파일 분석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.1 문서 구조 분석기를 통해 기본 분석 결과 저장 \n",
    "# layour_analtzer가 분석한 결과 -> json\n",
    "state_out = analyze_layout(analyzer, state)\n",
    "state.update(state_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'filepath': './papers/objectvla.pdf',\n",
       " 'batch_size': 10,\n",
       " 'split_filepaths': ['./papers/objectvla_0000_0009.pdf',\n",
       "  './papers/objectvla_0010_0010.pdf'],\n",
       " 'analyzed_files': ['./papers/objectvla_0000_0009.json',\n",
       "  './papers/objectvla_0010_0010.json']}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.2 문서에 대한 메타데이터 추출 \n",
    "# 논문 페이지 크기에 대한 파라메타 \n",
    "state_out = extract_page_metadata(state)\n",
    "state.update(state_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'filepath': './papers/objectvla.pdf',\n",
       " 'batch_size': 10,\n",
       " 'split_filepaths': ['./papers/objectvla_0000_0009.pdf',\n",
       "  './papers/objectvla_0010_0010.pdf'],\n",
       " 'analyzed_files': ['./papers/objectvla_0000_0009.json',\n",
       "  './papers/objectvla_0010_0010.json'],\n",
       " 'page_metadata': {0: {'size': [1275, 1650]},\n",
       "  1: {'size': [1275, 1650]},\n",
       "  2: {'size': [1275, 1650]},\n",
       "  3: {'size': [1275, 1650]},\n",
       "  4: {'size': [1275, 1650]},\n",
       "  5: {'size': [1275, 1650]},\n",
       "  6: {'size': [1275, 1650]},\n",
       "  7: {'size': [1275, 1650]},\n",
       "  8: {'size': [1275, 1650]},\n",
       "  9: {'size': [1275, 1650]},\n",
       "  10: {'size': [1275, 1650]}}}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.3 문서 구조와 내용에 대한 html 내용 추출\n",
    "# 페이지별 정보를 추출 \n",
    "state_out = extract_page_elements(state)\n",
    "state.update(state_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.4 문서 요소 별 tag 추출\n",
    "state_out = extract_tag_elements_per_page(state)\n",
    "state.update(state_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.5 페이지 번호 추출 \n",
    "state_out = page_numbers(state)\n",
    "state.update(state_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page:0, id:3, path: ./papers/objectvla/3.png\n",
      "page:3, id:33, path: ./papers/objectvla/33.png\n",
      "page:4, id:47, path: ./papers/objectvla/47.png\n",
      "page:4, id:50, path: ./papers/objectvla/50.png\n",
      "page:4, id:59, path: ./papers/objectvla/59.png\n",
      "page:4, id:63, path: ./papers/objectvla/63.png\n",
      "page:5, id:71, path: ./papers/objectvla/71.png\n",
      "page:10, id:121, path: ./papers/objectvla/121.png\n",
      "page:10, id:123, path: ./papers/objectvla/123.png\n"
     ]
    }
   ],
   "source": [
    "# 2.1 이미지를 추출하여 저장하고 위치를 저장 \n",
    "state_out = crop_image(state)\n",
    "state.update(state_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page:5, id:76, path: ./papers/objectvla/76.png\n",
      "page:6, id:85, path: ./papers/objectvla/85.png\n"
     ]
    }
   ],
   "source": [
    "# 2.2 표를 추출하여 저장하고 위치를 저장 \n",
    "state_out = crop_table(state)\n",
    "state.update(state_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.3 수식을 추출하여 저장하고 위치를 저장 \n",
    "state_out = crop_equation(state)\n",
    "state.update(state_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.4 텍스트를 추출하고 저장하여 위치를 저장 \n",
    "state_out = extract_page_text(state)\n",
    "state.update(state_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: '# ObjectVLA: End-to-End Open-World Object Manipulation\\nWithout Demonstration \\n 2025\\nFeb\\n28\\n[cs.RO]\\narXiv:2502.19250v2Minjie Zhu12∗ Yichen Zhu1∗† Jinming Li3 Zhongyi Zhou2\\nJunjie Wen2 Xiaoyu Liu3 Chaomin Shen2 Yaxin Peng3 Feifei Feng1\\n1Midea Group 2East China Normal University 3Shanghai University\\n∗Equal contribution †Corresponding authorFigure 1. A brief illustration of ObjectVLA. Conventional imitation learning focuses on tasks that involve both skills and objects. While\\nit performs well on seen objects and sometimes generalizes to similar ones (e.g., objects with changed colors), it typically fails with novel\\nobjects. By co-training with image-text data, our approach enables VLA models to generalize to any object present in the image-text\\ndataset. Additionally, users can capture object images, automatically generate image-text data, and fine-tune a pre-trained VLA model\\nwith minimal resources to learn manipulation on novel objects.',\n",
       " 1: '# Abstract \\n Imitation learning has proven to be highly effective in teach-\\ning robots dexterous manipulation skills. However, it typi-\\ncally relies on large amounts of human demonstration data,\\nwhich limits its scalability and applicability in dynamic,real-world environments. One key challenge in this context\\nis object generalization—where a robot trained to perform\\na task with one object, such as “hand over the apple,” strug-\\ngles to transfer its skills to a semantically similar but visu-\\nally different object, such as “hand over the peach.” This\\ngap in generalization to new objects beyond those in thesame category has yet to be adequately addressed in pre-\\nvious work on end-to-end visuomotor policy learning. In\\nthis paper, we present a simple yet effective approach for\\nachieving object generalization through Vision-Language-\\nAction (VLA) models, referred to as ObjectVLA. Our model\\nenables robots to generalize learned skills to novel objects\\nwithout requiring explicit human demonstrations for each\\nnew target object. By leveraging vision-language pair data,\\nour method provides a lightweight and scalable way to in-\\nject knowledge about the target object, establishing an im-\\nplicit link between the object and the desired action. We\\nevaluate ObjectVLA on a real robotic platform, demonstrat-\\ning its ability to generalize across 100 novel objects with a\\n64% success rate in selecting objects not seen during train-\\ning. Furthermore, we propose a more accessible method\\nfor enhancing object generalization in VLA models—using\\na smartphone to capture a few images and fine-tune the\\npre-trained model. These results highlight the effective-\\nness of our approach in enabling object-level generalization\\nand reducing the need for extensive human demonstrations,\\npaving the way for more flexible and scalable robotic learn-\\ning systems.',\n",
       " 2: '# 1. Introduction \\n Vision-language-action (VLA) models have emerged as\\na transformative paradigm for teaching robots dexterous\\nskills, enabling them to replicate human behavior and mas-\\nter complex tasks [3–5, 27, 37]. However, a critical limita-\\ntion persists: these models rely heavily on human demon-\\nstration data, which constrains their scalability and practi-\\ncality in dynamic real-world environments [17, 33, 34]. For\\ninstance, a robot trained to execute “hand over the apple” of-\\nten fails to generalize to analogous tasks like “hand over the\\npeach,” despite conceptual similarity. This underscores the\\nunresolved challenge of object generalization — adapting\\nlearned skills to novel, unseen objects — particularly when\\nsuch objects lie beyond the category of the teleoperated\\ntraining data. We name these objects as out-of-distribution\\n(OOD) objects.The core limitation stems from imitation learning’s ten-\\ndency to learn fixed mappings from instruction and vi-\\nsual input to action. When encountering objects ab-\\nsent from teleoperation data, the model lacks mecha-\\nnisms to associate the object’s name, visual features, and\\nlearned actions. To address this, we propose a framework\\nthat bridges visual-language semantics and robotic actions\\nthrough localization-aware reasoning.Our approach begins by curating a dataset of image-text\\npairs augmented with localization metadata (e.g., bounding\\nboxes). This dataset is co-finetuned with teleoperated robot\\ninteraction data, while the robot data itself is enriched with\\nlocalization-guided reasoning. By embedding localizationas a bridging representation, we create a unified pathway\\nbetween visual-language inputs and robotic actions. This\\nenables zero-shot object generalization: the model can rec-\\nognize and manipulate novel objects—even those absent\\nfrom robot training data—without task-specific retraining.We designed rigorous real-robot experiments to vali-\\ndate the generalization capabilities of our framework, Ob-\\njectVLA. In these trials, six objects are positioned at dis-\\ntinct locations (left or right side of a table), with configu-\\nrations spanning combinations of objects seen in robot in-\\nteraction data or vision-language data. The robot is tasked\\nwith the instruction “move to the object”, achieving a 100%\\nsuccess rate for in-domain objects. To stress-test gen-\\neralization, we evaluated 100 OOD objects, observing a\\n64% success rate. These experiments demonstrate that our\\nmethod adapts to diverse novel object types when trained\\nwith vision-language priors.The versatility of our approach is further demonstrated\\nacross diverse scenarios, including bin-picking and tasks re-\\nquiring composite skills like pushing and rotating. Notably,\\nour framework supports rapid adaptation to novel objects:\\nby collecting smartphone-captured images and performing\\nlightweight fine-tuning, the model generalizes to objects ab-\\nsent from the original dataset. These experiments under-\\nscore our method’s ability to reduce reliance on large-scale\\nhuman demonstrations while achieving robust object gener-\\nalization.Our primary contribution is a unified pipeline for in-\\ntegrating vision-language datasets with robot interaction\\ndata, enabling end-to-end object generalization. Through\\nsystematic evaluation, we validate the framework’s perfor-\\nmance on complex multi-stage tasks (e.g., bin-picking) and\\nmulti-skill manipulation (e.g., rotating, pushing), highlight-\\ning its universality. Despite some of the existing works,\\nsuch as RT-2 [5] and ECoT [37] giving a glimpse of\\nhow co-finetuning can achieve simple object generalization,\\nthey neither elucidate the underlying mechanism of achiev-\\ning such generalization nor address the boundary of their\\nmethodologies. In contrast, our approach — though sim-\\nple and straightforward — demonstrates that training VLA\\nmodels with a hybrid dataset of robot interaction data and\\nimage-text data significantly enhances generalization. This\\nlevel of generalization goes significantly beyond previously\\ndemonstrated end-to-end approaches. Crucially, our frame-\\nwork enables practical deployment: even a small set of\\nsmartphone images and brief fine-tuning suffices to adapt\\nthe model to novel objects, significantly advancing real-\\nworld robotic flexibility.',\n",
       " 3: '# 2. Related Work \\n Vision-language-action models for robot control. Re-\\ncent research has focused on developing generalist robot\\npolicies trained on increasingly expansive robot learningdatasets [9, 10, 16, 21, 26]. Vision-language-action mod-\\nels (VLAs) represent a promising approach for training\\nsuch generalist policies [3, 8, 17, 25, 27, 35, 38, 40].\\nVLAs adapt vision-language models (VLMs) [1, 7, 14, 22–\\n24, 32, 39, 41, 44], pre-trained on vast internet-scale im-\\nage and text data, for robotic control. This approach of-\\nfers several advantages: leveraging large vision-language\\nmodel backbones, with billions of parameters, provides the\\nnecessary capacity for fitting extensive robot datasets. Fur-\\nthermore, reusing weights pre-trained on internet-scale data\\nenhances the ability of VLAs to interpret diverse language\\ncommands and generalize to novel objects and environ-\\nments. However, current VLA models struggle to recog-\\nnize open-world objects when these objects absent from the\\nrobot interaction data [17, 34]. This is mainly due to VLMs\\nessentially “overwrites” its previously acquired knowledge\\nof open-world objects with robot-specific information.Generalization in robot learning. In the realm of robot\\nlearning, generalization, particularly object generalization,\\nremains a core challenge and active area of research. Many\\nworks leverage techniques such as domain randomiza-\\ntion [13], meta-learning [15, 29], retrieval-augmented gen-\\neration [43], extra modality [36, 45], and data augmenta-\\ntion to improve a robot’s ability to recognize and interact\\nwith novel objects unseen during training. For instance, do-\\nmain randomization methods [13, 31] randomize visual and\\nphysical parameters during simulation training to force the\\nagent to learn features invariant to these irrelevant details,\\nleading to better real-world generalization. Furthermore,\\nmeta-learning approaches [11] aim to train models that can\\nrapidly adapt to new objects with limited data, directly ad-\\ndressing the object generalization problem. Finally, data\\naugmentation methods [18, 19], enhance the diversity of the\\ntraining data, exposing the model to a wider range of object\\nappearances and orientations, thereby promoting robustness\\nand generalization to novel objects. There is also a field\\nof work using large language models or vision-language\\nmodels to do open-vocabulary manipulation [2, 20, 30, 42],\\ncombined with motion planning and robot learning meth-\\nods. However, these approaches involve separate modules\\nthat are trained independently for different components. To\\nthe best of our knowledge, this work represents the first\\nexploration of object generalization beyond specific cate-\\ngories within visuomotor policy learning.',\n",
       " 4: '# 3. Methodology \\n 3.1. Notation and MotivationGiven a set of expert demonstrations that contain complex\\nrobot skill trajectories, we want to learn a visuomotor pol-\\nicy π : {Or, Ir} (cid:55)→ A that maps the visual observations\\nor ∈ Or and the language instruction ir ∈ Ir to actions\\na ∈ A. The action changes accordingly when the lan-guage instruction and visual input change. The r denote the\\ndata in the human demonstration data. Typically, for each\\nlanguage instruction it contains robot skill such as ”push”\\nor ”pick up” and the target object, which is denoted as\\n{objr, skillr} ∈ ir. We then formally define the image-\\ntext data, where φ : {Ov, Iv} (cid:55)→ Lv, where we input the\\nimage ov ∈ Ov and give a language instruction iv ∈ Iv,\\nthe model is output with the corresponding answer lr ∈ Lv.\\nThe notation v denotes image-text data.In this work, we explore the generalization of objects,\\nfocusing on those that are not part of the robot interaction\\ndata but are present in image-text data.3.2. Data Constructionimage-text data construction. To explore the model’s abil-\\nity to generalize to novel objects, we constructed a diverse\\nimage-text dataset. For the visual component, we collected\\n100 distinct objects that are not included in the robot inter-\\naction objects. Specifically, using three cameras mounted\\non the robot (see Figure 2), we captured 20 images per ob-\\nject, covering various poses and orientations to ensure di-\\nversity. For the textual component, we employ a fixed tem-\\nplate, “Detecting the bounding box of object.”, as the ques-\\ntion, and the corresponding bounding box as the answer. In\\ntotal, our vision-language dataset comprises 2,000 image-\\ntext pairs.Reasoning data construction. We utilize localization\\nmetadata to bridge the gap between image-text data and\\nrobot data, as previously mentioned. To establish this im-\\nplicit link between image-text and action, we incorporate\\nlocalization metadata into the robot data. This section de-\\ntails how we construct reasoning with localization for robot\\ndata.For each task, we first identify target objects based on\\nthe language instructions. We then employ DinoX [28],\\na cutting-edge open-vocabulary object detector, to an-\\nnotate the bounding boxes of these objects. DinoX can\\ngenerate a bounding box given an object’s name. To\\nensure accuracy, we manually verify and correct any\\nerroneous bounding boxes produced by DinoX. Since\\nour workspace has two external camera views, which can\\nresult in different bounding boxes for the same object,\\nwe annotate only one (right camera in our experiments).\\nFollowing Qwen2-VL [32], we use a fixed template,\\n“<|object ref start|>{object}<|object ref end|><|box start|>\\n(x1, y1),(x2, y2)<|box end|>.”, to represent the localiza-\\ntion reasoning. This reasoning is generated before each\\naction and injected into the policy model through a learn-\\nable module. For a detailed explanation of this injection\\nmodule’s architecture, we refer readers to DiVLA [33],\\nthe base model used in our experiments. An example of\\nconstructed image-text data is at Figure 3.Figure 2. Robot setup and examples for real-world manipulation tasks. We evaluate ObjectVLA with 4 skills on a Franka robot arm\\nequipped with two external Zed cameras and a Realsense 435i wrist camera.3.3. Training Strategy and Implementation DetailsFor our experiments, we adopt diffusion-based VLA, a\\nwidely used class of Vision-Language-Action (VLA) mod-\\nels exemplified by methods like π0 [3] and TinyVLA [34].\\nWe select diffusion-based VLA over auto-regressive alter-\\nnatives due to its significantly faster inference speed, a\\ncritical advantage for real-time robotic applications (see\\nFAST [27] for a detailed comparison). Specifically, we\\nutilize DiVLA [33], a representative VLA architecture, co-\\ntrain on a hybrid dataset comprising robot interaction data\\nand the vision-language corpus. To balance task-specific\\nadaptation and semantic generalization, we maintained a\\n10:1 data ratio (robot-to-image-text data) across all tasks.\\nThis ratio empirically proved sufficient for robust object\\ngeneralization, aligning with prior findings on the bene-\\nfits of co-training for VLA capabilities. Notably, increas-\\ning the proportion of robot data beyond this ratio led to a\\ndecline in in-domain task success rates. We hypothesize\\nthis stems from the limited capacity of the 2B-parameter\\nDiVLA model compared to larger architectures like ECoT\\n(7B) [37] and RT-2 (55B) [5], which can better absorb\\ndomain-specific data without overfitting.',\n",
       " 5: '# 4. Experiments \\n In this section, we examine the effectiveness of ObjectVLA\\nfor object generalization in embodied control. In sec-\\ntion 4.1, we verify the effectiveness of our method in object\\ngeneralization. In section 4.2 and 4.3, we illustrate how our\\nmodel transfers skills to objects not present in robot inter-\\naction data but included in the vision-language corpus. In\\nsection 4.4, we show that even a small set of smartphone\\nimages and brief fine-tuning can effectively adapt the pre-\\ntrained model to novel objects.\\nReal robot setup. All experiments are conducted on aFranka robot [12] equipped with a 7-degree-of-freedom arm\\nand a gripper. We use two external ZED cameras and a wrist\\nRealsense 435i camera to obtain real-world visual informa-\\ntion. Our real-world robot setup is illustrated in Figure 2.4.1. Validating Object GeneralizationIn this section, we conduct rigorous experiments to verify\\nthe object generalization capability of our method. We be-\\ngin by describing the experimental setup and evaluation cri-\\nteria. Next, we evaluate ObjectVLA on both in-distribution\\nand out-of-distribution objects. Finally, we explore several\\ninteresting observations related to object generalization.4.1.1. Experimental SetupTo verify the object generalization capability, we begin with\\na simple yet effective task, “Move to the object.”. In this\\ntask, we position objects on both sides of the robot, ensur-\\ning that each side has at least three objects on the table. The\\nmodel is required to move toward the target object based on\\nthe given instruction. These objects are randomly chosen\\nfrom a diverse set. For in-distribution (ID) evaluation, ob-\\njects are only selected from the robot’s training data. And,\\nfor out-of-distribution evaluation, objects are randomly se-\\nlected from either the robot’s training data or the vision-\\nlanguage data. A complete list of objects from both datasets\\nis provided in the Appendix.\\nEvaluation criterion. We evaluate each object over 4 trials,\\nwith the target area’s side switching every two trials. We\\nconsider the model to have successfully recognized a novel\\nobject if and only if it moved toward the target object in\\nall four trials. This criterion ensures that the model cannot\\nachieve success simply by chance.\\nExperimental Results. Figure 5 presents the real-world\\nexperimental results for the ”Move” task. Our ObjectVLA\\nachieves a 100% success rate in ID evaluation. In the stress-Example of Image-Text Paired Data',\n",
       " 6: '# Photo taken by cameras from robot \\n ',\n",
       " 7: '# Question Detecting the bounding box of stick. \\n Answer <|object_ref_start|>stick<|object_ref_end|>\\n<|box_start|>(546,106),(557, 143)<|box_end|>.',\n",
       " 8: '# Question Detecting the bounding box of yellow dragon. \\n Answer <|object_ref_start|>yellow dragon<|object_ref_end| <|object_ref_start|>Pikachu<|object_ref_end|>><|box_start|>(232,150),(389, 331)<|box_end|>. <|box_start|>(503, 523),(702, 694)<|box_end|>.Figure 3. Example of constructed image-text data. Left: Photo taken by the robot’s camera. Right: Object captured with a smartphone.test evaluation, our model successfully recognizes 64% of\\nobjects that are not present in the robot interaction data,\\nconfirming the effectiveness of co-training robot data with\\nlocalization metadata.\\nAblation study. To further understand our method’s ef-\\nfectiveness, we conducted an ablation study. We found\\nthat object generalization relies heavily on two key fac-\\ntors: first, explicitly linking vision and language to action\\nthrough bounding boxes. This provides a direct connection\\nbetween the visual object, its linguistic description, and the\\nrequired manipulation. Second, a reasoning process for the\\nrobot data should be designed that mirrors the structure of\\nvision-language pair data. This allows the model to leverage\\nthe rich information encoded in pre-trained vision-language\\nmodels.To analyze the impact of these factors, we removed the\\nreasoning module for robot data and eliminated bounding\\nboxes for vision-language data. The VLA model is then co-\\nfinetuned with vision-language data and evaluated using the\\nsame criteria and test settings as our full method.As illustrated in Figure 5, the model without bounding\\nboxes achieves only a 19% success rate in OOD evaluation,\\nrepresenting a significant performance decline compared to\\nour method, despite achieving a 100% success rate in thePhoto taken by smart-phoneQuestion Detecting the bounding box of brown toy cat.Answer<|object_ref_start|> toy cat <|object_ref_end|>\\n<|box_start|> (421, 272),(657, 599)<|box_end|>.Question Detecting the bounding box of pikachu.AnswerID test. This suggests that without explicit grounding and\\na structured reasoning process, the model struggles to dif-\\nferentiate objects in vision-language data, leading to confu-\\nsion about object-instruction correspondence and appropri-\\nate action selection.4.1.2. More ObservationsCan VLA recognize unseen objects if only trained with\\nteleoperated data? To further assess the importance of\\nvision-language data, we evaluated a VLA model trained\\nexclusively on robot data, without any vision-language co-\\nfinetuning. As shown in Figure 5, this model (DiVLA)\\nachieved 8% accuracy, which is almost equivalent to ran-\\ndom guessing. This stark outcome highlights the critical\\nrole of vision-language data in multimodal understanding.While the VLA model’s backbone is pre-trained on\\ninternet-scale vision-language data, focusing solely on\\nrobot data during training leads to catastrophic forgetting.\\nThe model essentially “overwrites” its previously acquired\\nknowledge of visual concepts with robot-specific infor-\\nmation, hindering its ability to comprehend multimodal\\nscenes. Consequently, even objects encountered during pre-\\ntraining, such as Pikachu, remain unrecognizable to the\\nVLA model without vision-language co-finetuning.Figure 4. Example Objects Used in Experiments. Left: Objects present in the robot training data. Right: Examples of novel objects, not\\npresent in the robot data, but included in the image-text co-training dataset (see Appendix for a comprehensive list).Chart Type: bar\\n  In-Distribution Out-of-Distribution\\n item_01 100% 8%Figure 5. Validation experiments on object generalization. Our\\nmethod achieved the best performance in both the in-distribution\\ntest setup and under visual changes. Each object is evaluated\\nacross 4 trials. We report the number of objects that were cor-\\nrectly identified in all four trials.Table 1. Experimental Results for rotate and push skills. Our\\nproposed ObjectVLA achieves high performance on both 5 in-\\ndistribution objects and 20 out-of-distribution objects. Each object\\nis evaluated with three trials. We report the number of success tri-\\nals.4.2. Combining with More SkillsWhile the previous section employed a simple “move to”\\ndemonstration to validate the fundamental approach of our\\nmethod, this section expands the evaluation to encompass\\nmore complex skills, specifically ”push” and ”rotate.” This\\nbroader assessment aims to demonstrate the generalizability\\nof our method and its applicability beyond the ”move to”\\ntask.Experimental setup. In this experimental setup, we placedthree objects in front of the robot: one on the center, one\\non the right, and one on the left. The robot is instructed\\nto either ”rotate the object counterclockwise” or ”push the\\nobject forward,” as illustrated in Figure 2. Following pre-\\nvious setup, we evaluate the model’s performance for both\\nin-distribution (ID) and out-of-distribution (OOD) objects.\\nRecognizing that some objects are inherently unsuitable for\\nrotation or pushing actions (e.g., dishes), we conducted ex-\\nperiments on a curated set of 5 ID objects and 20 OOD ob-\\njects. For each object in a skill, 40 demonstrations were\\ncollected, resulting in a total of 400 demonstrations.Implementation details. We train one model for each skill\\nto ensure that the model focuses more on understanding the\\nobjects rather than multi-task learning. We use the same\\nimage-text data of “move” task. Following established pro-\\ntocols from our prior work, this image-text dataset trained\\nconcurrently with the demonstration data for comprehen-\\nsive evaluation. Each object was tested with 3 trials. In\\ntotal, 150 trials were conducted. The training setting is pro-\\nvided in Appendix.Results. As shown in Table 1, our method achieved high\\nsuccess rates on the robot interaction objects for both ro-\\ntate and push skills. Analysis of the failed “rotate” tri-\\nals revealed that the primary cause is the model’s inabil-\\nity to grasp the target object securely. When evaluating\\nperformance on out-of-distribution (OOD) objects, we ob-\\nserved a decrease in task completion rates compared to in-\\ndistribution objects, as expected. However, the model still\\nsuccessfully completed nearly two-thirds of the trials. No-\\ntably, in most failure cases, the model did not incorrectly\\nidentify the target object but rather failed to execute the skill\\ncompletely. This was particularly evident in the “rotate” tri-\\nals, where successful execution hinges on a secure grasp,\\na challenging requirement for unseen objects. Neverthe-\\nless, these experiments strongly support the claim that Ob-\\njectVLA can transfer learned skills, beyond basic pick and\\nplace, to novel objects within the framework we have devel-\\noped. The results underscore the potential of ObjectVLA\\nfor generalized robotic manipulation, capable of adaptingTable 2. Experimental results for bin picking. Our proposed\\nObjectVLA achieves high performance on both 11 in-distribution\\nobjects and 50 out-of-distribution objects, with each object evalu-\\nated across 3 trials. We report the number of successful trials over\\ntotal trials.to new objects and tasks beyond its initial training.4.3. Instruction-Driven Bin PickingTo further evaluate ObjectVLA, we conducted experiments\\nin a more practical scenario: end-to-end instruction-driven\\nbin-picking. Unlike prior works (e.g., GR-2 [6] and Di-\\nVLA [33]) that execute bin-picking tasks without spe-\\ncific semantic instructions—typically limited to generic ac-\\ntions like transferring all objects from one container to an-\\nother—we focus on a significantly more challenging set-\\nting [6, 33]. In our experiments, the robot is required\\nto identify and retrieve a specific target object based on\\nnatural language instructions (e.g., ”Pick the hexagonal\\nbolt from the bin”). This scenario elevates the complex-\\nity of conventional bin-picking tasks by integrating cross-\\nmodal understanding (vision-to-language alignment) and\\nfine-grained object discrimination that have multiple ob-\\njects in the scene. Notably, the objects are randomly placed\\non the panel, which is a large area. Not only does the model\\nneed to figure out the object’s position, but also needs to be\\naware of its pose.Implementation details. We collected new data within this\\nenvironment. For robot interaction data, we collected 600\\npick-and-place trajectories using the same ”seen” objects as\\nin previous experiments. For image-text data, we used half\\nthe number of objects from previous experiments, captur-\\ning 20 images of each. We compared our method against\\nOpenVLA, a state-of-the-art VLA model, reporting success\\nrates for both in-distribution and out-of-distribution objects.\\nEvaluation consisted of three trials per object, totaling 183\\ntrials per method. In each trial, at least two objects were\\nrandomly placed on the plate, and the model was instructed\\nto pick and place a specific object according to the given\\ninstruction.Results. Table 2 presents our experimental results. Bin\\npicking, requiring object retrieval from random positions\\nand poses, poses a significant challenge even for in-\\ndistribution objects. OpenVLA achieves a success rate of\\nonly 42.4% for in-distribution objects, significantly less\\nthan half. Surprisingly, it still completed roughly 10% of\\ntrials with out-of-distribution objects. This is likely due\\nto some test objects sharing attributes with training ob-\\njects (e.g., bread resembling a muffin, a green mug differ-100Y-Axis: Success Rate Chart Type: bar\\n  Green Orange\\n item_01 80% 90%Figure 6. Experimental results for smartphone captured ob-\\njects and trained by continual learning. We test two new ob-\\njects. We took pictures of these two objects via smartphone and\\ncontinually trained them on a pre-trained model. Each object was\\nevaluated across 10 trials. We report the success rate for each ob-\\nject.ing only in color from a brown training mug). In contrast,\\nour method successfully completed 87 of 150 trials, includ-\\ning many completely novel objects, a 46.7% improvement\\nover OpenVLA. This further emphasizes the necessity of\\nco-training with both robot interaction and image-text data\\nfor effective object generalization.4.4. Cheap Object Generalization via Smart-Phone\\nPictures and Continual LearningThe previous section demonstrated that our proposed co-\\ntraining strategy enables the imitation learning method to\\ngeneralize to any object by constructing corresponding\\nimage-text data for each object. This approach significantly\\nenhances the model’s ability to handle a broader range of\\nobjects, without requiring extensive retraining. However,\\nthere are two key limitations that need to be addressed.First, when a new object is introduced, the model must\\nbe trained from scratch to incorporate the new object into its\\nunderstanding. This process can be highly cost-inefficient,\\nas it involves retraining the model every time a novel ob-\\nject is added. In real-world applications, where objects are\\nfrequently introduced or changed, this limitation could sig-\\nnificantly slow down deployment and increase operational\\ncosts. Second, our current image data is collected using\\ncameras mounted on the robot, which ensures that there is\\nno visual gap between the images captured by the robot’s\\ncameras and the images input into the model. This setup\\nworks well in controlled environments but presents chal-\\nlenges in real-world scenarios. For instance, in order to\\ncapture the same images as the robot sees, you would need\\nto replicate the exact camera positions and angles of the\\nrobot’s setup. This is not only cumbersome but also expen-sive, as it requires building an identical system with match-\\ning camera views. Moreover, in environments where the\\nrobot is mobile or the scene is dynamic, maintaining con-\\nsistent camera alignment becomes even more difficult.Therefore, in this section, we test a simpler and more\\ncost-effective approach: using a smart-phone camera to col-\\nlect images from various perspectives. The model is then\\ncontinuously trained on the pre-trained weights using this\\nmore accessible data collection method. As shown in Fig-\\nure 6, we test with two objects: Pikachu and a brown toy cat.\\nFor each object, we capture 21 images and follow the same\\ndata construction pipeline discussed earlier. We train these\\nobjects in a bin-picking environment. Our results demon-\\nstrate that the model is able to recognize and successfully\\ngrasp the objects with a high success rate, 80% for Pikachu\\nand 90% success rate for the toy cat. More importantly, we\\nonly need to continue training the model for 1 epoch. Be-\\ncause the collected data size is small, the training process\\ncan be extremely fast and can be finished up to ten min-\\nutes. This validates the effectiveness of our approach, show-\\ning that simple smartphone image collection combined with\\ncontinuous learning enables open-world object manipula-\\ntion in an end-to-end model. This experiment demonstrates\\nthat our method is flexible and cost-effective, making it a\\nplug-and-play solution for existing VLA models, enabling\\nthem to generalize to virtually any object.',\n",
       " 9: '# 5. Conclusion \\n In this work, we present ObjectVLA, a Vision-Language-\\nAction framework that addresses object generalization\\nin robotic manipulation. By integrating vision-language\\ndatasets with robot interaction data, our method establishes\\na unified pipeline that bridges semantic understanding and\\nphysical action execution. This enables zero-shot gener-\\nalization to over 100 novel objects with a 64% success\\nrate, even when objects differ in category, appearance, or\\nfine-grained attributes (e.g., color, shape). Our framework\\ndemonstrates that lightweight co-training with image-text\\npriors and localization-aware reasoning can unlock robust\\ncross-modal alignment. Key to our success is the abil-\\nity to adapt rapidly to real-world scenarios: using just a\\nfew smartphone-captured images and quick continual fine-\\ntuning, robots generalize to unseen objects without costly\\nhuman demonstrations. We validate our approach across\\ndiverse tasks—including bin-picking, rotating and push-\\ning—showcasing its versatility and practicality. Our results\\nhighlight a path toward scalable robotic learning systems\\nthat reduce dependence on large-scale teleoperation data\\nwhile maintaining high performance.',\n",
       " 10: '# 6. Limitation \\n There are still a number of limitations in this work. Specifi-\\ncally, the image-text data was collected by the authors using\\neither a robot-mounted camera or a smartphone. While we\\nhave not yet explored the feasibility of leveraging internet-\\nsourced image-text data, it presents an intriguing avenue for\\nfuture research. Specifically, investigating the necessary de-\\ngree of visual similarity between internet images and target\\nobjects for effective skill transfer would be valuable. Our\\nprimary focus here is to introduce a novel pipeline that en-\\nables deep learning models to transfer skills to new objects\\nwithout explicit demonstrations. Determining the limits\\nof this transferability, particularly concerning the permissi-\\nble visual gap between training and target objects, remains\\nan open question for future investigation. Currently, our\\nmethod struggles to generalize to novel backgrounds and\\nlighting conditions. We believe the visual gap between our\\ncollected image-text data and the robot’s operational envi-\\nronment contributes to this challenge. Bridging this gap to\\nimprove generalization is a key focus for future develop-\\nment.',\n",
       " 11: '# References \\n [1] Marah Abdin, Jyoti Aneja, Hany Awadalla, Ahmed Awadal-\\nlah, Ammar Ahmad Awan, Nguyen Bach, Amit Bahree,\\nArash Bakhtiari, Jianmin Bao, Harkirat Behl, et al. Phi-3\\ntechnical report: A highly capable language model locally\\non your phone. arXiv preprint arXiv:2404.14219, 2024. 3\\n[2] Michael Ahn, Anthony Brohan, Noah Brown, Yevgen Cheb-\\notar, Omar Cortes, Byron David, Chelsea Finn, Chuyuan Fu,\\nKeerthana Gopalakrishnan, Karol Hausman, et al. Do as i\\ncan, not as i say: Grounding language in robotic affordances.\\narXiv preprint arXiv:2204.01691, 2022. 3\\n[3] Kevin Black, Noah Brown, Danny Driess, Adnan Esmail,\\nMichael Equi, Chelsea Finn, Niccolo Fusai, Lachy Groom,\\nKarol Hausman, Brian Ichter, Szymon Jakubczak, Tim\\nJones, Liyiming Ke, Sergey Levine, Adrian Li-Bell, Mohith\\nMothukuri, Suraj Nair, Karl Pertsch, Lucy Xiaoyang Shi,\\nJames Tanner, Quan Vuong, Anna Walling, Haohuan Wang,\\nand Ury Zhilinsky. π0: A vision-language-action flow model\\nfor general robot control, 2024. 2, 3, 4\\n[4] Anthony Brohan, Noah Brown, Justice Carbajal, Yevgen\\nChebotar, Joseph Dabis, Chelsea Finn, Keerthana Gopalakr-\\nishnan, Karol Hausman, Alex Herzog, Jasmine Hsu, et al.\\nRt-1: Robotics transformer for real-world control at scale.\\narXiv preprint arXiv:2212.06817, 2022.\\n[5] Anthony Brohan, Noah Brown, Justice Carbajal, Yevgen\\nChebotar, Xi Chen, Krzysztof Choromanski, Tianli Ding,\\nDanny Driess, Avinava Dubey, Chelsea Finn, et al. Rt-2:\\nVision-language-action models transfer web knowledge to\\nrobotic control. arXiv preprint arXiv:2307.15818, 2023. 2,\\n4\\n[6] Chi-Lam Cheang, Guangzeng Chen, Ya Jing, Tao Kong,\\nHang Li, Yifeng Li, Yuxiao Liu, Hongtao Wu, Jiafeng Xu,\\nYichu Yang, et al. Gr-2: A generative video-language-action\\nmodel with web-scale knowledge for robot manipulation.\\narXiv preprint arXiv:2410.06158, 2024. 7[7] Zhe Chen, Jiannan Wu, Wenhai Wang, Weijie Su, Guo Chen,\\nSen Xing, Muyan Zhong, Qinglong Zhang, Xizhou Zhu,\\nLewei Lu, et al. Internvl: Scaling up vision foundation mod-\\nels and aligning for generic visual-linguistic tasks. In Pro-\\nceedings of the IEEE/CVF Conference on Computer Vision\\nand Pattern Recognition, pages 24185–24198, 2024. 3\\n[8] Cheng Chi, Siyuan Feng, Yilun Du, Zhenjia Xu, Eric\\nCousineau, Benjamin Burchfiel, and Shuran Song. Diffu-\\nsion policy: Visuomotor policy learning via action diffusion.\\narXiv preprint arXiv:2303.04137, 2023. 3\\n[9] Sudeep Dasari, Oier Mees, Sebastian Zhao, Mohan Kumar\\nSrirama, and Sergey Levine. The ingredients for robotic dif-\\nfusion transformers. arXiv preprint arXiv:2410.10088, 2024.\\n3\\n[10] Hao-Shu Fang, Hongjie Fang, Zhenyu Tang, Jirong Liu,\\nChenxi Wang, Junbo Wang, Haoyi Zhu, and Cewu Lu.\\nRh20t: A comprehensive robotic dataset for learning diverse\\nskills in one-shot. arXiv preprint arXiv:2307.00595, 2023. 3\\n[11] Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model-\\nagnostic meta-learning for fast adaptation of deep networks.\\nIn International conference on machine learning, pages\\n1126–1135. PMLR, 2017. 3\\n[12] Sami Haddadin. The franka emika robot: A standard plat-\\nform in robotics research. IEEE Robotics & Automation\\nMagazine, 2024. 4\\n[13] Stephen James, Paul Wohlhart, Mrinal Kalakrishnan, Dmitry\\nKalashnikov, Alex Irpan, Julian Ibarz, Sergey Levine, Raia\\nHadsell, and Konstantinos Bousmalis. Sim-to-real via sim-\\nto-sim: Data-efficient robotic grasping via randomized-\\nto-canonical adaptation networks. In Proceedings of the\\nIEEE/CVF conference on computer vision and pattern\\nrecognition, pages 12627–12637, 2019. 3\\n[14] Siddharth Karamcheti, Suraj Nair, Ashwin Balakrishna,\\nPercy Liang, Thomas Kollar, and Dorsa Sadigh. Prismatic\\nvlms: Investigating the design space of visually-conditioned\\nlanguage models. arXiv preprint arXiv:2402.07865, 2024. 3\\n[15] Rituraj Kaushik, Timoth´ee Anne, and Jean-Baptiste Mouret.\\nFast online adaptation in robotics through meta-learning em-\\nbeddings of simulated priors. In 2020 IEEE/RSJ Interna-\\ntional Conference on Intelligent Robots and Systems (IROS),\\npages 5269–5276. IEEE, 2020. 3\\n[16] Alexander Khazatsky, Karl Pertsch, Suraj Nair, Ash-\\nwin Balakrishna, Sudeep Dasari, Siddharth Karamcheti,\\nSoroush Nasiriany, Mohan Kumar Srirama, Lawrence Yun-\\nliang Chen, Kirsty Ellis, et al. Droid: A large-scale\\nin-the-wild robot manipulation dataset. arXiv preprint\\narXiv:2403.12945, 2024. 3\\n[17] Moo Jin Kim, Karl Pertsch, Siddharth Karamcheti, Ted Xiao,\\nAshwin Balakrishna, Suraj Nair, Rafael Rafailov, Ethan Fos-\\nter, Grace Lam, Pannag Sanketi, Quan Vuong, Thomas Kol-\\nlar, Benjamin Burchfiel, Russ Tedrake, Dorsa Sadigh, Sergey\\nLevine, Percy Liang, and Chelsea Finn. Openvla: An\\nopen-source vision-language-action model. arXiv preprint\\narXiv:2406.09246, 2024. 2, 3\\n[18] Ilya Kostrikov, Denis Yarats, and Rob Fergus. Image aug-\\nmentation is all you need: Regularizing deep reinforce-\\nment learning from pixels. arXiv preprint arXiv:2004.13649,\\n2020. 3[19] Misha Laskin, Kimin Lee, Adam Stooke, Lerrel Pinto, Pieter\\nAbbeel, and Aravind Srinivas. Reinforcement learning with\\naugmented data. Advances in neural information processing\\nsystems, 33:19884–19895, 2020. 3\\n[20] Jacky Liang, Wenlong Huang, Fei Xia, Peng Xu, Karol\\nHausman, Brian Ichter, Pete Florence, and Andy Zeng. Code\\nas policies: Language model programs for embodied control.\\nIn 2023 IEEE International Conference on Robotics and Au-\\ntomation (ICRA), pages 9493–9500. IEEE, 2023. 3\\n[21] Fanqi Lin, Yingdong Hu, Pingyue Sheng, Chuan Wen, Ji-\\nacheng You, and Yang Gao. Data scaling laws in imitation\\nlearning for robotic manipulation, 2024. 3\\n[22] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee.\\nImproved baselines with visual instruction tuning. arXiv\\npreprint arXiv:2310.03744, 2023. 3\\n[23] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee.\\nVisual instruction tuning. In Thirty-seventh Conference on\\nNeural Information Processing Systems, 2023.\\n[24] Haoyu Lu, Wen Liu, Bo Zhang, Bingxuan Wang, Kai\\nDong, Bo Liu, Jingxiang Sun, Tongzheng Ren, Zhuoshu Li,\\nHao Yang, et al. Deepseek-vl: towards real-world vision-\\nlanguage understanding. arXiv preprint arXiv:2403.05525,\\n2024. 3\\n[25] Dantong Niu, Yuvan Sharma, Giscard Biamby, Jerome\\nQuenum, Yutong Bai, Baifeng Shi, Trevor Darrell, and Roei\\nHerzig. Llarva: Vision-action instruction tuning enhances\\nrobot learning. arXiv preprint arXiv:2406.11815, 2024. 3\\n[26] Abby O’Neill, Abdul Rehman, Abhinav Gupta, Abhiram\\nMaddukuri, Abhishek Gupta, Abhishek Padalkar, Abraham\\nLee, Acorn Pooley, Agrim Gupta, Ajay Mandlekar, et al.\\nOpen x-embodiment: Robotic learning datasets and rt-x\\nmodels. arXiv preprint arXiv:2310.08864, 2023. 3\\n[27] Karl Pertsch, Kyle Stachowicz, Brian Ichter, Danny Driess,\\nSuraj Nair, Quan Vuong, Oier Mees, Chelsea Finn, and\\nSergey Levine. Fast: Efficient action tokenization for vision-\\nlanguage-action models. arXiv preprint arXiv:2501.09747,\\n2025. 2, 3, 4\\n[28] Tianhe Ren, Yihao Chen, Qing Jiang, Zhaoyang Zeng, Yuda\\nXiong, Wenlong Liu, Zhengyu Ma, Junyi Shen, Yuan Gao,\\nXiaoke Jiang, et al. Dino-x: A unified vision model for open-\\nworld object detection and understanding. arXiv preprint\\narXiv:2411.14347, 2024. 3\\n[29] Gerrit Schoettler, Ashvin Nair, Juan Aparicio Ojea, Sergey\\nLevine, and Eugen Solowjow. Meta-reinforcement learn-\\ning for robotic industrial insertion tasks. In 2020 IEEE/RSJ\\nInternational Conference on Intelligent Robots and Systems\\n(IROS), pages 9728–9735, 2020. 3\\n[30] Austin Stone, Ted Xiao, Yao Lu, Keerthana Gopalakrish-\\nnan, Kuang-Huei Lee, Quan Vuong, Paul Wohlhart, Sean\\nKirmani, Brianna Zitkovich, Fei Xia, et al. Open-world ob-\\nject manipulation using pre-trained vision-language models.\\narXiv preprint arXiv:2303.00905, 2023. 3\\n[31] Josh Tobin, Rachel Fong, Alex Ray, Jonas Schneider, Woj-\\nciech Zaremba, and Pieter Abbeel. Domain randomization\\nfor transferring deep neural networks from simulation to the\\nreal world. In 2017 IEEE/RSJ International Conference on\\nIntelligent Robots and Systems (IROS), pages 23–30, 2017.\\n3[32] Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan,\\nJinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin\\nGe, et al. Qwen2-vl: Enhancing vision-language model’s\\nperception of the world at any resolution. arXiv preprint\\narXiv:2409.12191, 2024. 3, 10\\n[33] Junjie Wen, Minjie Zhu, Yichen Zhu, Zhibin Tang, Jinming\\nLi, Zhongyi Zhou, Chengmeng Li, Xiaoyu Liu, Yaxin Peng,\\nChaomin Shen, et al. Diffusion-vla: Scaling robot founda-\\ntion models via unified diffusion and autoregression. arXiv\\npreprint arXiv:2412.03293, 2024. 2, 3, 4, 7, 10\\n[34] Junjie Wen, Yichen Zhu, Jinming Li, Minjie Zhu, Kun Wu,\\nZhiyuan Xu, Ning Liu, Ran Cheng, Chaomin Shen, Yaxin\\nPeng, et al. Tinyvla: Towards fast, data-efficient vision-\\nlanguage-action models for robotic manipulation. arXiv\\npreprint arXiv:2409.12514, 2024. 2, 3, 4\\n[35] Junjie Wen, Yichen Zhu, Jinming Li, Zhibin Tang, Chaomin\\nShen, and Feifei Feng. Dexvla: Vision-language model with\\nplug-in diffusion expert for general robot control. arXiv\\npreprint arXiv:2502.05855, 2025. 3\\n[36] Weirui Ye, Fangchen Liu, Zheng Ding, Yang Gao, Oleh Ry-\\nbkin, and Pieter Abbeel. Video2policy: Scaling up manip-\\nulation tasks in simulation through internet videos. arXiv\\npreprint arXiv:2502.09886, 2025. 3\\n[37] Michał Zawalski, William Chen, Karl Pertsch, Oier Mees,\\nChelsea Finn, and Sergey Levine. Robotic control via\\nembodied chain-of-thought reasoning. arXiv preprint\\narXiv:2407.08693, 2024. 2, 4\\n[38] Michał Zawalski, William Chen, Karl Pertsch, Oier Mees,\\nChelsea Finn, and Sergey Levine. Robotic control via\\nembodied chain-of-thought reasoning. arXiv preprint\\narXiv:2407.08693, 2024. 3\\n[39] Han Zhao, Min Zhang, Wei Zhao, Pengxiang Ding, Siteng\\nHuang, and Donglin Wang. Cobra: Extending mamba to\\nmulti-modal large language model for efficient inference.\\narXiv preprint arXiv:2403.14520, 2024. 3\\n[40] Zhongyi Zhou, Yichen Zhu, Minjie Zhu, Junjie Wen, Ning\\nLiu, Zhiyuan Xu, Weibin Meng, Ran Cheng, Yaxin Peng,\\nChaomin Shen, et al. Chatvla: Unified multimodal un-\\nderstanding and robot control with vision-language-action\\nmodel. arXiv preprint arXiv:2502.14420, 2025. 3\\n[41] Minjie Zhu, Yichen Zhu, Xin Liu, Ning Liu, Zhiyuan Xu,\\nChaomin Shen, Yaxin Peng, Zhicai Ou, Feifei Feng, and\\nJian Tang. Mipha: A comprehensive overhaul of multi-\\nmodal assistant with small language models. arXiv preprint\\narXiv:2403.06199, 2024. 3\\n[42] Yifeng Zhu, Arisrei Lim, Peter Stone, and Yuke Zhu. Vision-\\nbased manipulation from single human video with open-\\nworld object graphs. arXiv preprint arXiv:2405.20321,\\n2024. 3\\n[43] Yichen Zhu, Zhicai Ou, Xiaofeng Mou, and Jian Tang.\\nRetrieval-augmented embodied agents. In Proceedings of\\nthe IEEE/CVF Conference on Computer Vision and Pattern\\nRecognition, pages 17985–17995, 2024. 3\\n[44] Yichen Zhu, Minjie Zhu, Ning Liu, Zhiyuan Xu, and Yaxin\\nPeng. Llava-phi: Efficient multi-modal assistant with small\\nlanguage model. In Proceedings of the 1st International\\nWorkshop on Efficient Multimedia Computing under Limited,\\npages 18–22, 2024. 3[45] Yichen Zhu, Zhicai Ou, Feifei Feng, and Jian Tang.\\nAny2policy: Learning visuomotor policy with any-modality.\\nAdvances in Neural Information Processing Systems, 37:\\n133518–133540, 2025. 3',\n",
       " 12: '# 7. Appendix \\n 7.1. Evaluation MetricsFor real robot task, we record the percentage of trials where\\nthe robot successfully completes the assigned task. This is\\na fundamental metric for any robot experiment. Multiple\\ntrials are conducted for the evaluation.7.2. Implementation DetailsAll experiments were conducted on eight NVIDIA A800\\nGPUs using the Adam optimizer with a constant learn-\\ning rate of 2e-5 and a global batch size of 128. Train-\\ning proceeded for 50,000 steps, with the final checkpoint\\nselected based on validation performance. Unless other-\\nwise stated, the ratio of robot data to visual-text data was\\n10:1. Empirically, we observed that increasing the propor-\\ntion of robot data significantly degraded manipulation per-\\nformance. Our base model is DiVLA[33] with a Qwen2-\\nVL-2B backbone [32]. As our focus is developing a co-\\ntraining method for novel object generalization, we retained\\nthe original model architecture.7.3. Example of Objects Used in ExperimentsWe provide a comprehensive list of out-of-distrbution ob-\\njects and names that we used for training and evaluation,\\nwhich are shown in Figure 7 and Figure 8.Figure 7. The out-of-distribution objects used in our experiments (part 1).Figure 8. The out-of-distribution objects used in our experiments (part 2).'}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state[\"texts\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "115"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(state['html_content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pdf파일 이름과 같은 이름으로 마크다운 파일 저장\n",
    "\n",
    "pdf_file = state[\"filepath\"]  # PDF 파일 경로\n",
    "output_folder = os.path.splitext(pdf_file)[0]  # 출력 폴더 경로 설정\n",
    "filename = os.path.basename(pdf_file).split('.')[0]\n",
    "\n",
    "md_output_file1 = save_results(output_folder, filename, state['html_content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./papers/objectvla/objectvla.md'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "md_output_file1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 생성 \n",
    "- 번역 \n",
    "- 요약"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_model = 'gpt-4o-mini'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_summary_chain = get_chain(selected_model, summary_prompt)\n",
    "paper_summary_chain = get_chain(selected_model, map_prompt)\n",
    "trans_chain = get_translator(selected_model, trans_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 텍스트 요약 생성\n",
    "state_out = create_text_summary(text_summary_chain, state)\n",
    "state.update(state_out)\n",
    "\n",
    "state_out = map_reduce_summary(paper_summary_chain, state)\n",
    "state.update(state_out)\n",
    "\n",
    "# 요약 번역\n",
    "trans_chain = get_translator(selected_model, trans_prompt)\n",
    "state_out = create_text_trans_summary(trans_chain, state)\n",
    "state.update(state_out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 원본 번역\n",
    "# trans_chain = get_translator(selected_model, trans_prompt)\n",
    "# state_out = create_text_trans(trans_chain, state)\n",
    "# state.update(state_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image 요약 생성 \n",
    "state_out = create_image_summary_data_batches(state)\n",
    "state.update(state_out)\n",
    "\n",
    "state_out = create_image_summary(state)\n",
    "state.update(state_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Table 요약 생성 \n",
    "state_out = create_table_summary_data_batches(state)\n",
    "state.update(state_out)\n",
    "\n",
    "\n",
    "state_out = create_table_summary(state)\n",
    "state.update(state_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Equation 요약 생성 \n",
    "state_out = create_equation_summary_data_batches(state)\n",
    "state.update(state_out)\n",
    "\n",
    "state_out = create_equation_summary(state)\n",
    "state.update(state_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4 표를 다시 마크다운 표 생성 \n",
    "state_out = create_table_markdown(state)\n",
    "state.update(state_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 수식 이미지 처리\n",
    "\n",
    "cnt = 1\n",
    "for key, value in state['equation_summary'].items():\n",
    "    equation_html = f\"<p id='{key}_1' data-category='equation' style='font-size:14px'>{value}</p>\"\n",
    "    state['html_content'].insert(cnt+int(key), equation_html)\n",
    "    cnt+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HTML 파일이 ./papers/objectvla/objectvla.html에 저장되었습니다.\n",
      "Markdown 파일이 ./papers/objectvla/objectvla.md에 저장되었습니다.\n"
     ]
    }
   ],
   "source": [
    "# 생성 내용 분석 파일에 덮어써서 저장\n",
    "md_output_file = save_results(output_folder, filename, state['html_content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "output_file = output_folder + '/'+ filename + \"_analy.json\"    \n",
    "\n",
    "# pdf구조를 json으로 저장 \n",
    "with open(output_file, \"w\", encoding='utf-8') as file:\n",
    "    json.dump(state, file, ensure_ascii=False)\n",
    "\n",
    "# 분석 번역 요약 과정에서 생긴 json 파일 제거 \n",
    "for del_file in state['split_filepaths'] + state['analyzed_files']:\n",
    "    os.remove(del_file)\n",
    "    \n",
    "# 과정에서 생긴 html 파일 제거 \n",
    "# os.remove('.'.join(file_path.split('.')[:-1]) + f'/{filename}.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # vectordb 만들기 \n",
    "# build_db(state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 요약 내용 \n",
    "\n",
    "- 원본 요약 내용 \n",
    "- 한국어 요약 내용 \n",
    "\n",
    "구분해서 하자"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 원본 영어 번역\n",
    "\n",
    "원본 md 파일을 읽어서 한국어로 통번역 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./papers/objectvla/objectvla.md\n"
     ]
    }
   ],
   "source": [
    "original_paper_md = f\"{output_folder}/{filename}.md\"\n",
    "new_docs = load_and_split(original_paper_md)\n",
    "translated_paragraph = ['# ' + new_docs[0].metadata['Header 1']] + trans_chain.batch(new_docs[1:])\n",
    "combined_content = \"\\n\".join(translated_paragraph)\n",
    "md_output = markdown(combined_content)\n",
    "\n",
    "trans_paper_md = f\"{output_folder}/{filename}_trans.md\"\n",
    "with open(trans_paper_md, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(md_output)\n",
    "\n",
    "print(trans_paper_md)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 영어 요약"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file = output_folder + '/'+ filename + \"_analy.json\"    \n",
    "with open(output_file, \"r\", encoding='utf-8') as f:\n",
    "    json_data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ObjectVLA: End-to-End Open-World Object Manipulation\n",
      "Without Demonstration\n",
      "Abstract\n",
      "1. Introduction\n",
      "2. Related Work\n",
      "3. Methodology\n",
      "4. Experiments\n",
      "Photo taken by cameras from robot\n",
      "Question Detecting the bounding box of stick.\n",
      "Question Detecting the bounding box of yellow dragon.\n",
      "5. Conclusion\n",
      "6. Limitation\n"
     ]
    }
   ],
   "source": [
    "markdown_contents = []  # 마크다운 내용을 저장할 빈 리스트\n",
    "    \n",
    "\n",
    "names = json_data['section_names']\n",
    "for i, page in enumerate(json_data['texts_summary'].keys()):\n",
    "    page = int(page)\n",
    "    if names[page] == 'References':\n",
    "        continue\n",
    "    print(names[page])\n",
    "    text_summary = json_data['texts_summary'][str(page)]\n",
    "    section_title = f'# {names[page]}'\n",
    "    if i==0:\n",
    "        text_summary = json_data['paper_summary']\n",
    "    \n",
    "    markdown_contents.append(section_title)\n",
    "    markdown_contents.append(text_summary)\n",
    "    \n",
    "    for image_summary_data_batch in json_data['image_summary_data_batches']:\n",
    "        if image_summary_data_batch['page'] == page:\n",
    "            img_file = image_summary_data_batch['image'].split('/')[-1]\n",
    "            img_name = os.path.basename(img_file).split('.')[0]\n",
    "            # markdown_result = html_to_markdown_table(json_data['images_summary'][img_name])\n",
    "            \n",
    "            # 이미지와 테이블 마크다운을 리스트에 추가\n",
    "            markdown_contents.append(f'\\n ![{img_name}]({img_file}) \\n')\n",
    "            # markdown_contents.append(f'\\n {markdown_result} \\n')\n",
    "            \n",
    "    for table_summary_data_batch in json_data['table_summary_data_batches']:\n",
    "        if table_summary_data_batch['page'] == page:\n",
    "            table_img_file = table_summary_data_batch['table'].split('/')[-1]\n",
    "            table_text = table_summary_data_batch['text']\n",
    "            \n",
    "            table_img_name = os.path.basename(table_img_file).split('.')[0]\n",
    "            # markdown_result = html_to_markdown_table(json_data['tables_summary'][table_img_name])\n",
    "            \n",
    "            # 테이블과 텍스트도 리스트에 추가\n",
    "            markdown_contents.append(f'\\n ![{table_img_name}]({table_img_file}) \\n')\n",
    "            # markdown_contents.append(f'\\n {markdown_result} \\n')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summary 마크다운 저장하기 \n",
    "\n",
    "# 리스트에 저장된 마크다운 내용을 하나의 파일로 저장\n",
    "markdown_file_path = f'{output_folder}/{filename}_summary_en.md'\n",
    "with open(markdown_file_path, 'w', encoding='utf-8') as f:\n",
    "    f.write('\\n'.join(markdown_contents))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 한국어 요약"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(output_file, \"r\", encoding='utf-8') as f:\n",
    "    json_data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = json_data['section_names']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ObjectVLA: End-to-End Open-World Object Manipulation\n",
      "Without Demonstration\n",
      "Abstract\n",
      "1. Introduction\n",
      "2. Related Work\n",
      "3. Methodology\n",
      "4. Experiments\n",
      "Photo taken by cameras from robot\n",
      "Question Detecting the bounding box of stick.\n",
      "Question Detecting the bounding box of yellow dragon.\n",
      "5. Conclusion\n",
      "6. Limitation\n"
     ]
    }
   ],
   "source": [
    "markdown_contents = []\n",
    "\n",
    "for i, page in enumerate(json_data['texts_summary'].keys()):\n",
    "    page = int(page)\n",
    "    if names[page] == 'References':\n",
    "        continue\n",
    "    print(names[page])\n",
    "    if i == 0:\n",
    "        text_summary = json_data['paper_trans_summary']\n",
    "    else:\n",
    "        text_summary = json_data['texts_trans_summary'][str(page)]\n",
    "        \n",
    "    section_title = f'# {names[page]}'\n",
    "    \n",
    "    markdown_contents.append(section_title)\n",
    "    markdown_contents.append(text_summary)\n",
    "    \n",
    "    for image_summary_data_batch in json_data['image_summary_data_batches']:\n",
    "        if image_summary_data_batch['page'] == page:\n",
    "            img_file = image_summary_data_batch['image'].split('/')[-1]\n",
    "            img_name = os.path.basename(img_file).split('.')[0]\n",
    "            \n",
    "            # 이미지와 테이블 마크다운을 리스트에 추가\n",
    "            markdown_contents.append(f'![{img_name}]({img_file})')\n",
    "\n",
    "            \n",
    "    for table_summary_data_batch in json_data['table_summary_data_batches']:\n",
    "        if table_summary_data_batch['page'] == page:\n",
    "            table_img_file = table_summary_data_batch['table'].split('/')[-1]\n",
    "            table_text = table_summary_data_batch['text']\n",
    "            table_img_name = os.path.basename(table_img_file).split('.')[0]\n",
    "           \n",
    "            # 테이블과 텍스트도 리스트에 추가\n",
    "            markdown_contents.append(f'![{table_img_name}]({table_img_file})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "markdown_file_path = f'{output_folder}/{filename}_summary_ko.md'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(markdown_file_path, 'w', encoding='utf-8') as f:\n",
    "    f.write('\\n'.join(markdown_contents))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 채팅하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "paper",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
